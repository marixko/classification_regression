{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Machine Learning Hands-on",
      "provenance": [],
      "collapsed_sections": [
        "WDMJHAujxi7U",
        "dZbQHFM6o5lk",
        "At1Wujug-ElJ",
        "HUztj4xWCmFi",
        "wD5hIoaDKIhj",
        "Yu8aGNZrLSyW",
        "H8Rweo2_LirI",
        "QRL_7q8aL1sl",
        "mUagJbKEL7D6",
        "noxaXAXQMQND",
        "7u7CDn2GMUpt",
        "_Fa-eAeVNMIS",
        "rbVK9FzFNVns"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHxouXUiCNjY",
        "colab_type": "text"
      },
      "source": [
        "This material was created by:\n",
        "\n",
        "\n",
        "*   Erik Vinicius de Lima - erik.vini@usp.br (Ph.D. student, IAG-USP)\n",
        "*   Lilianne Nakazono - lilianne.nakazono@usp.br (Ph.D. candidate, IAG-USP)\n",
        "*   Maria Luisa Buzzo - maria.buzzo@usp.br (M.Sc. student, IAG-USP)\n",
        "\n",
        "Feel free to contact us if you have any question and/or suggestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OurmpomdDqjH",
        "colab_type": "text"
      },
      "source": [
        "--- \n",
        "\n",
        "**HOW TO GET THIS NOTEBOOK:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7neJlPvIuBg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Go to the machine_learning_IXLAPIS repository on GitHub\n",
        "\n",
        "\n",
        "**Link:** https://github.com/marixko/machine_learning_IXLAPIS\n",
        "\n",
        "**Shortened link:** https://bit.ly/2V1OdFJ\n",
        "\n",
        "\n",
        "- Running in jupyter notebook\n",
        "\n",
        "  - Download the Machine_Learning_Hands_on.ipynb file;\n",
        "  - In your terminal, go to the directory where the Machine_Learning_Hands_on.ipynb is;\n",
        "  - Type jupyter-notebook;\n",
        "  - Inside jupyter, open the Machine_Learning_Hands_on.ipynb file\n",
        "\n",
        "- Running in Google Colab (1)\n",
        "  - Click in the Machine_Learning_Hands_on.ipynb file \n",
        "  - Click in \"Open in Colab\" button\n",
        "  - Go to File > Save a copy in Drive\n",
        "\n",
        "- Running in Google Colab (2)\n",
        "\n",
        "  - Open Google Colab: https://colab.research.google.com/\n",
        "  - Go to File > Open Notebook\n",
        "  - Select the GITHUB tab\n",
        "  - Enter the link: https://github.com/marixko/machine_learning_IXLAPIS.git\n",
        "  - It will show you the Machine_Learning_Hands_on.ipynb file, click the \"Open notebook in a new tab\" icon\n",
        "  - Go to File > Save a copy in Drive \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z59YHXPKuNp",
        "colab_type": "text"
      },
      "source": [
        "**SOME TRICKS:**\n",
        "\n",
        "To run the code in jupyter or Google Colab:\n",
        "- to run a cell and stay in the same cell: ```ctrl+enter```\n",
        "- to run a cell and go to the next one: ```shift+enter``` - this is what I will be using.\n",
        "- to create new cells: \n",
        "  - jupyter: ```b+b```\n",
        "  - Google Colab: ```ctrl+m b```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knl71lTK1MhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell in the beginning of the three Machine Learning hands-on sessions! \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3SFZ99cDsdM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Descriptive Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cqyqdFZpyc",
        "colab_type": "text"
      },
      "source": [
        "The ultimate goal of this hands-on session is to learn how to read, visualize and understand the data and the magics of seaborn and scikit-learn. Hold on to that thought and let's do this! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9MjWaEBVJYN",
        "colab_type": "text"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-h-IIZ6uwJ",
        "colab_type": "text"
      },
      "source": [
        "We will use data containing information of house pricing of California district from a 1990 census. The information are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDYG77wP6PfX",
        "colab_type": "text"
      },
      "source": [
        "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
        "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
        "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
        "4. totalRooms: Total number of rooms within a block\n",
        "5. totalBedrooms: Total number of bedrooms within a block\n",
        "6. population: Total number of people residing within a block\n",
        "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
        "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bAkIJp-lUze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's read the dataset:\n",
        "\n",
        "california_housing = pd.read_table(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "# ascii use delim_whitespace=True instead of sep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3te2noE0LOS_",
        "colab_type": "text"
      },
      "source": [
        "## Statistics and Data Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf96aOavUK39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking statistics of all columns\n",
        "california_housing.describe()\n",
        "\n",
        "# If you want to check the name of the columns:\n",
        "# list(california_housing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZy2NxdsFka6",
        "colab_type": "text"
      },
      "source": [
        "It is essential to understand the data.\n",
        " - Counts: missing bands;\n",
        " - mean: if there are wrong values (due to observations or human error), very sensitive to outliers;\n",
        " - std: if it is too high, it is a proxy for problems... \n",
        " - min: if there are negative values where it shouldn't have.\n",
        "\n",
        " Allows to check, in general, if there is anything wrong with the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj5w93uVm0zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eywKrZECUUnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PgdV3oBU0ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.hist(figsize=(15,15), bins=30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFV8lYvVI7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(california_housing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0NKYIs9rOf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(california_housing.corr(), cmap='bwr', vmax=1, vmin=-1, center=0, square=True, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pV89fvQunhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", \n",
        "                        alpha=0.3, s=california_housing[\"population\"]/100, \n",
        "                        label=\"population\", c=\"median_house_value\", \n",
        "                        cmap=plt.get_cmap(\"jet\"), colorbar=True, \n",
        "                        figsize=(15,7)) \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpFOqdyuEl0t",
        "colab_type": "text"
      },
      "source": [
        "The boxplot is one of the most important analysis we can make of our data, specially to identify outliers.\n",
        "\n",
        "It is defined by 5 values:\n",
        "\n",
        " - median;\n",
        " - 1st quartile: 25% of the data;\n",
        " - 3rd quartile: 75% of the data;\n",
        " - Superior Limit: $min\\{max(data): Q3 + 1.5(Q3-Q1)\\}$;\n",
        " - Inferior Limit: $max\\{min(data): Q1 - 1.5(Q3-Q1)\\}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVqxRBdi61me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing[['median_house_value']].boxplot(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbNokz9iLE5p",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "A density estimator is an algorithm to model the probability distribution of a dataset. For one dimensional data, we use the most common simple density estimator: the histogram. A histogram divides the data into discrete bins, counts the number of points that fall in each bin, and then visualizes the results.\n",
        "\n",
        "\n",
        "- How to plot one dimensional histograms?\n",
        "\n",
        "For example, let's start visualizing a bimodal distribution, for example the longitude.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPBUHojQB5me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = plt.hist(california_housing['longitude'], bins=30, density=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzSq9sTYeMcV",
        "colab_type": "text"
      },
      "source": [
        "The factor 'normed=True' normalizes the data, so we can see in the y-axis is the probability and not the counts.\n",
        "Since we are dealing with probabilities, this distribution should sum to 1, as we can check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJZ46jXvedPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "density, bins, patches = hist\n",
        "widths = bins[1:] - bins[:-1]\n",
        "(density * widths).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBG3JcRkMJXL",
        "colab_type": "text"
      },
      "source": [
        "This was a good choice of number of bins, but what do we do when this is not true?\n",
        "\n",
        "Actually, this is one of the issues with using a regular histogram as a density estimator: the choice of bin size and location can lead to representations that have qualitatively different features. \n",
        "\n",
        "- Let us try different numbers of bins to see what happens...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boljp7t9GpAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
        "                       sharex=True, sharey=True)\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].hist(california_housing['longitude'], bins=100, density=True)\n",
        "ax[1].hist(california_housing['longitude'], bins=5, density=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhz_VdCofQFS",
        "colab_type": "text"
      },
      "source": [
        "On the left, the histogram makes clear that this is a bimodal (or even with more modes - undersmoothing) distribution. On the right, we see a unimodal distribution (oversmoothing). Without seeing the preceding code, you would probably not guess that these two histograms were built from the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKj-6gLb60Mp",
        "colab_type": "text"
      },
      "source": [
        "To solve this, it is always better to plot as a function of the density. We can think of a histogram as a stack of blocks, where we stack one block within each bin on top of each point in the dataset.\n",
        "\n",
        "Let's use a standard normal curve at each point instead to represent each block:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LNPi3Tk7PzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "density = sum(norm(xi).pdf(california_housing['longitude']) for xi in california_housing['longitude'])\n",
        "\n",
        "plt.fill_between(california_housing['longitude'], density, alpha=0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1g59-VKgE5x",
        "colab_type": "text"
      },
      "source": [
        "This last plot is an example of a kernel density estimation in one dimension: it uses a Gaussian kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSMe5kFP7lNL",
        "colab_type": "text"
      },
      "source": [
        "- The Kernel Density Estimation (KDE) is a non-parametric method to estimate the Probability Distribution Function of a given dataset.\n",
        "- KDE is a data smoothing problem, where inferences about the data population are made.\n",
        "- In python, the function kde is part of the packages `seaborn` and `scikit-learn`.\n",
        "\n",
        "The free parameters of kernel density estimation are the kernel, which specifies the shape of the distribution placed at each point, and the kernel bandwidth, which controls the size of the kernel at each point.\n",
        "\n",
        "Here we introduce the beauty of using Scikit-Learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tx7VPbV9-td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "# instantiate and fit the KDE model\n",
        "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
        "kde.fit(california_housing['longitude'][:, None])\n",
        "\n",
        "# score_samples returns the log of the probability density\n",
        "logprob = kde.score_samples(california_housing['longitude'][:, None])\n",
        "\n",
        "plt.fill_between(california_housing['longitude'], np.exp(logprob), alpha=0.5)\n",
        "plt.ylim(-0.02, 0.22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477R0iBU-PNZ",
        "colab_type": "text"
      },
      "source": [
        "The choice of bandwidth within KDE is extremely important to finding a suitable density estimate.\n",
        "\n",
        "- The badwidth is a smoothing parameter and the better it is defined, the better the data visualization will be.\n",
        "- How do we determine the best bandwidth then?\n",
        "\n",
        "-> Cross-Validation, where the basic idea is to construct an estimate of $F(h)$ and then to select $h$ to minimize this estimate.\n",
        "\n",
        "We can use K-Fold or Leave-One-Out methods:\n",
        " - Leave-One-Out: Leave-One-Out (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. This cross-validation procedure does not waste much data as only one sample is removed from the training set.\n",
        " - K-Fold: K-Fold divides all the samples in $k$ groups of samples, called folds (if $k=n$, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using $k-1$ folds, and the fold left out is used for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnsy2pE6AQMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "bandwidths = 10 ** np.linspace(-1, 1, 10)\n",
        "grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n",
        "                    {'bandwidth': bandwidths},\n",
        "                    cv=KFold())\n",
        "\n",
        "grid.fit(california_housing['longitude'][:, None]);\n",
        "\n",
        "grid.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrh-LN_XD9C",
        "colab_type": "text"
      },
      "source": [
        "What about 2-D distributions? What is the best way to show the distribution of the data?\n",
        "\n",
        "Let's compare a simple scatter plot with a 2-D histogram using hexbin (the hexagonal binning routine).\n",
        "\n",
        "- Using `seaborn` we can visualize the marginal distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FgYUlF5AjZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'],alpha=0.3);\n",
        "with sns.axes_style(\"white\"):\n",
        "  sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'], kind=\"hex\", color=\"k\");\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62IqShoiXmJE",
        "colab_type": "text"
      },
      "source": [
        "From the previous plots, we can already see the great advantage of using a \"hex\" distribution in the data visualization\n",
        "Instead of overlapping, the plotting window is split in several hexbins, and the number of points per hexbin is counted. But it can be even better... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfUBrik3XyHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'], kind=\"kde\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDMJHAujxi7U",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "Now it's time to play around with astronomical data. For this exercise, you will use a sample data from S-PLUS. \n",
        "\n",
        "1. Read splus_laplata.txt with pandas\n",
        "2. Use df.info() to check the name and type of all columns\n",
        "3. Use df.describe().T to check statistics of your data \n",
        "4. Use df.column.value_counts() to check how many stars, quasars and galaxies are in this data\n",
        "5. Store in another variable called \"gal_splus\" a dataframe containing only galaxies. Check df.describe().T again, now only for galaxies.\n",
        "6. What is the filter that have highest standard deviation in magnitudes?\n",
        "7. Use df.column.boxplot() on the answer from Item 6. \n",
        "\n",
        "Now playing with distributions...\n",
        "\n",
        "Using the galaxies dataset:\n",
        "\n",
        "1. Retrieve two different colour distributions (1D histogram) of the data, one for the objects with u-r > 2.22 and one for u-r < 2.22. What is the comparison between the two distributions? What types of objects are more frequent in the local universe? \n",
        "2. What is the best bandwidth (h value) that describes both of the distributions?\n",
        "3. Plot the colour-colour diagram u-r vs. g-i  using scatter, hexbin and kde. Is there any bimodality? If so, what is the (astronomical) explanation for it?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS3Q3IhGzLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read splus data with pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tLJHEspG1ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the statistics: describe, info..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNbiwyxqG_Y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# value_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbCrfFU3HEoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select only galaxies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5jH7CclHN92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# boxplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEkFsxtzHPWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using the sample of galaxies, exclude objects with missing bands"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJGoEn5XHUVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate u-r and g-i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPbaHSbHZAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create variables to store the values greater and smaller than 2.22"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4tIhMxdHj1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using cross validation find the bandwidth that maximize the data visualization (do this for only one of the above distributions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKYY5pWDH6tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot u-r vs g-i using seaborn to visualize the bimodality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbQHFM6o5lk",
        "colab_type": "text"
      },
      "source": [
        "## Cluster analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU3fHXz3YYEj",
        "colab_type": "text"
      },
      "source": [
        "There are several ways to visualize different distributions present in the data.\n",
        "- The main idea of  clustering is: diminish the intra-group variance and increase the variance between groups.\n",
        "\n",
        "The most common method is the K-means. But what is this? \n",
        "\n",
        "- K-Means is an unsupervised learning method. K-means works iteratively by defining random centroids to clusters, trying to find the centroid that minimizes the distance of the points to the center of the defined sphere.\n",
        "\n",
        "Let's first create a dataset with clutering regions. `Scikit-learn` again!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_78irQ6ZDxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# create blobs\n",
        "data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n",
        "\n",
        "# create np array for data points\n",
        "points = data[0] # shape (n_samples,n_features)\n",
        "loc_cluster = data[1] # shape (n_samples) - to which cluster the point belongs.\n",
        "\n",
        "# create scatter plot\n",
        "plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\n",
        "plt.xlim(-15,15)\n",
        "plt.ylim(-15,15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is87rsMp6aT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# create kmeans object\n",
        "kmeans = KMeans(n_clusters=4)# fit kmeans object to data\n",
        "kmeans.fit(points)# print location of clusters learned by kmeans object\n",
        "print('Cluster Centers:', kmeans.cluster_centers_)# save new clusters for chart\n",
        "y_km = kmeans.fit_predict(points)\n",
        "\n",
        "plt.scatter(points[y_km ==0,0], points[y_km == 0,1], s=30, marker='^')\n",
        "plt.scatter(points[y_km ==1,0], points[y_km == 1,1], s=30, marker='s')\n",
        "plt.scatter(points[y_km ==2,0], points[y_km == 2,1], s=30, marker='o')\n",
        "plt.scatter(points[y_km ==3,0], points[y_km == 3,1], s=30, marker='x')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3kYwIoH0fH",
        "colab_type": "text"
      },
      "source": [
        "In this case, K-Means has worked very well and retrieved all the features that we were expecting. But this is not always true.\n",
        "\n",
        "By using this iterative manner, K-Means is a highly sensitive to outliers and initial conditions method.\n",
        " - The usual approach, therefore, is to run K-Means a lot of times in the same dataset to actually be sure that our results are reliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1Wujug-ElJ",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "For this exercise, you will use the same sample data from S-PLUS as before. \n",
        "\n",
        "1. Plot the same colour-colour diagram (u-r vs. g-i) as before, but now try to identify the different distributions using the K-Means method.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNBXhihgEi4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the S-PLUS data here:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iptMZvTjEpI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select only galaxies:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uj2-tJmEvTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exclude the objects with missing values:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqDd2O_xEz6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate u-r and g-i:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZQP1zBqE6yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run KMeans in the data using 2 clusters:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhXk3GAs9qhs",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk2gPHLuD2lI",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality reduction is the idea of reducing the number of random variables by considering a set of principle variables. There are several methods to do this: Principal Component Analysis, Local Linear Embedding ....\n",
        "\n",
        "Let's start with the easiest one: PCA (linear projection of the data)\n",
        "\n",
        "PCA is a method based on linear algebra, and for this reason it is considered a 'linear' method. By using orthogonal transformations, it finds \"new\" axes on the directions of maximum variance, called the Principal Components. Thus it allows the selection of the Principal Components that best describe the data.\n",
        "\n",
        "This method can be powerful, but often misses important non-linear structure in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axvpcG0M6fsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal');\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "print('PCA Variance:', pca.explained_variance_)\n",
        "\n",
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    linewidth=4,\n",
        "                    shrinkA=0, shrinkB=0,color='limegreen')\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSkK-5t6n0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", X_pca.shape)\n",
        "\n",
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], marker='x', alpha=0.8)\n",
        "plt.axis('equal');\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF92O9afL8gL",
        "colab_type": "text"
      },
      "source": [
        "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.\n",
        "\n",
        "Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.\n",
        "\n",
        "LLE: Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.\n",
        "\n",
        "The standard LLE algorithm comprises three stages:\n",
        "1. Nearest Neighbors Search.\n",
        "2. Weight Matrix Construction.\n",
        "3. Partial Eigenvalue Decomposition. \n",
        "\n",
        "Modified LLE: multiple weight vectors in each neighborhood.\n",
        "\n",
        "ISOMAP: Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points.\n",
        "The Isomap algorithm comprises three stages:\n",
        "1. Nearest neighbor search.\n",
        "2. Shortest-path graph search.\n",
        "3. Partial eigenvalue decomposition.\n",
        "\n",
        "Let's see an example of the differences between many manifold methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq67oUS16rrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "from sklearn import manifold, datasets\n",
        "\n",
        "# Next line to silence pyflakes. This import is needed.\n",
        "Axes3D\n",
        "\n",
        "n_points = 1000\n",
        "X, color = datasets.make_s_curve(n_points, random_state=0)\n",
        "n_neighbors = 10\n",
        "n_components = 2\n",
        "\n",
        "# Create figure\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
        "             % (1000, n_neighbors), fontsize=14)\n",
        "\n",
        "# Add 3d scatter plot\n",
        "ax = fig.add_subplot(251, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
        "ax.view_init(4, -72)\n",
        "\n",
        "# Set-up manifold methods\n",
        "LLE = partial(manifold.LocallyLinearEmbedding,\n",
        "              n_neighbors, n_components, eigen_solver='auto')\n",
        "\n",
        "methods = OrderedDict()\n",
        "methods['LLE'] = LLE(method='standard')\n",
        "methods['LTSA'] = LLE(method='ltsa')\n",
        "methods['Hessian LLE'] = LLE(method='hessian')\n",
        "methods['Modified LLE'] = LLE(method='modified')\n",
        "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
        "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
        "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
        "                                           n_neighbors=n_neighbors)\n",
        "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
        "                                 random_state=0)\n",
        "\n",
        "# Plot results\n",
        "for i, (label, method) in enumerate(methods.items()):\n",
        "    t0 = time()\n",
        "    Y = method.fit_transform(X)\n",
        "    t1 = time()\n",
        "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
        "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
        "    ax.xaxis.set_major_formatter(NullFormatter())\n",
        "    ax.yaxis.set_major_formatter(NullFormatter())\n",
        "    ax.axis('tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUztj4xWCmFi",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "For this exercise, you will use the same sample data from S-PLUS as before. \n",
        "\n",
        "1. Plot the colour-magnitude diagram of u-r vs. r and apply the PCA method. Do the same for other colours that you may find interesting.\n",
        "2. Do the same thing using the LLE method. Compare the results with the PCA.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYef2csFg26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the table, exclude objects with missing values (you don't need to extract only galaxies now)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujcPQeRFqe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you will need to standize your features onto unit scale (mean = 0 and variance = 1), since it is not created from normal distributions. \n",
        "#For this:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "features = ['color','r']\n",
        "# Separating out the features\n",
        "x = df.loc[:, features].values\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3QLH2TdGD_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run pca with n_components = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n50TWX9GZDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run the standard LLE with n_component = 1, n_neighbors = 10, eigen_solver='auto' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umTGoYBiNWxL",
        "colab_type": "text"
      },
      "source": [
        "### <font color='blue'>**RECOMMENDED BIBLIOGRAPHY**</font>\n",
        "\n",
        "1. Manifold in Scikit-Learn: https://scikit-learn.org/stable/modules/manifold.html.\n",
        "2. PCA Analysis: https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c.\n",
        "3. PCA Theory: Lever et al. 2017 (Nature Methods).\n",
        "4. K-Means Clustering: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1.\n",
        "5. Other methods of clustering: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68.\n",
        "6. Python Statistics and Data Visualization: https://realpython.com/python-statistics/.\n",
        "7. Cross-Validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "7. Prof. Laerte (:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YH2w25sfRin",
        "colab_type": "text"
      },
      "source": [
        "This is the end of my class! Thank you (:\n",
        "\n",
        "---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtNFPq_OT-tl"
      },
      "source": [
        "# Classification with Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6JdDSdgT67r",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHseXsBJT67s"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wZ3rzZLjT67t"
      },
      "source": [
        "The data set we are going to use in this part of the tutorial consists of **50 samples from each of three species of Iris** (*Iris setosa*, *Iris virginica* and *Iris versicolor*). **Four features** were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6NfkqTErT67t"
      },
      "source": [
        "<img src=\"https://thegoodpython.com/assets/images/iris-species.png\" width=900/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ob7_Nk0T67t"
      },
      "source": [
        "Let's store the useful information in a dataframe called **iris**. The real classification for each row (i.e., for each flower) will be stored in another variable called **target**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZOyNr-2bT67u",
        "colab": {}
      },
      "source": [
        "target = pd.DataFrame(datasets.load_iris().target, columns=['target'])\n",
        "iris = pd.DataFrame(datasets.load_iris().data, columns=datasets.load_iris().feature_names) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE44R4YTZL57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MnFeK6xg71S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#change columns\n",
        "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ez2rDI09T67w"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZy4C1SbT67w"
      },
      "source": [
        "This is the most important part when working with Machine Learning. It is not as simple as a cake recipe, but these few steps will already give you a better understanding of your data. Decisions must be taken from there, case by case. \n",
        "\n",
        "\n",
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "---\n",
        "\n",
        "STEP 1: Check if your dataset is ok \n",
        "- [ ] Check names of the columns -> `list(df)` or `print(df.columns)`\n",
        "- [ ] Check type of each column  -> `df.dtypes`\n",
        " ...or simply do `df.info()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "STEP 2: Check statistics\n",
        "- [ ] Statistical summary -> `df.describe().T` \n",
        "- [ ] Check outliers -> `df.column.boxplot()` \n",
        "\n",
        "(+ other analyses based on data visualization )\n",
        "\n",
        "---\n",
        "\n",
        "STEP 3: Check missing values\n",
        "- [ ] Check presence of NaNs -> `df.isna().sum()`\n",
        "- [ ] Drop NaNs, if necessary (not always the best way to deal with it!) -> `df = df.dropna()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "STEP 4: Check correlations\n",
        "- [ ] Correlation matrix -> `df.corr()`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRtx_jW6T67x"
      },
      "source": [
        "## Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6fLszWQ3T67y"
      },
      "source": [
        "In classification problems, we validate models through cross-validation. \n",
        "Here we are going to sample our data into training and validation sets in a very simple way called **Holdout method**, instead of K-Fold. \n",
        "\n",
        "In Holdout method we just sample X% of the data as training set and another (100-X)% as validation set. There is no rule for choosing X, but we usually use X=70 or X=75. \n",
        "However, we highly recommend that you use K-Fold in your works, instead of Holdout. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euuWevVfAoEf",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/holdout.png\"/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOGgfz6AAsMM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Note:** The iris dataset is very small, thus we will not consider a validation set and we will simply work with only a training and a testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rboqykjcT676",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris,target,test_size=0.3,random_state=42)\n",
        "print(len(X_test), len(X_train)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pnaJrskT678",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our dataset\n",
        "target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rlF6OzQT67-",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our test set\n",
        "y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AjYks7DAT67_",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our training set\n",
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_8k7H98T68B"
      },
      "source": [
        "Did you notice that we are sampling a different distribution from our initial dataset? We have more CLASS 0 than others in our testing set. Also, in our training set, we have less of CLASS 0. \n",
        "\n",
        "In order to maintain the initial proportion, we do a **stratified sampling**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6gqFrSET68B",
        "colab": {}
      },
      "source": [
        "# train_test_split with stratify parameter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9B9IHHWT68D"
      },
      "source": [
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "- [x] Split your data by classes frequences with stratified sampling \n",
        "- [ ] Check again the frequencies of classes 0, 1 and 2 from our training and testing sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KeQE7HdIT68J"
      },
      "source": [
        "## Training and Validating models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ppbmh4-WT68J"
      },
      "source": [
        "There are several classification methods you can use, each of them has its own pros and cons, depending on your science goals and on your dataset. \n",
        "\n",
        "Regardless of the algorithm you choose, keep in mind that you have to:\n",
        "\n",
        "- Check the algorithm assumptions of your data (e.g. linearity) \n",
        "- Check what each parameter of your model (also known as hyperparameter) does. This is important for you to be able to refine your model fitting\n",
        "- Check which features might be useful for your problem\n",
        "\n",
        "What you can validate through cross-validation:\n",
        "\n",
        "- Which algorithm to use\n",
        "- Which features to use\n",
        "- Values of hyperparameters \n",
        "\n",
        "We will give you an example using Decision Trees and Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yUrlVoPiT68J"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM-ezWFQnG5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting model\n",
        "clf_DT = DecisionTreeClassifier(random_state=42)\n",
        "clf_DT.fit(X_train, y_train.target.ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FmFW_xVnPGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting\n",
        "y_pred = clf_DT.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ZmF-lKkT68K",
        "colab": {}
      },
      "source": [
        "#Creating confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woTkliQ7napA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting confusion matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "fig = sns.heatmap(matrix, annot=True, square=True, cbar=False, cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels', size=18)\n",
        "plt.ylabel('True Labels',size=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9XM-V9_8T68L",
        "colab": {}
      },
      "source": [
        "#Print metrics\n",
        "print(metrics.classification_report\n",
        "      (y_test,y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4fpupHvqT68N"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhTDumugoEEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting model\n",
        "clf_RF = RandomForestClassifier(random_state=42)\n",
        "clf_RF.fit(X_train, y_train.target.ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXfATXoaoJdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting\n",
        "y_pred = clf_RF.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5DQfC3woLEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m_zgZZEtT68N",
        "colab": {}
      },
      "source": [
        "#Plotting confusion matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "fig = sns.heatmap(matrix, annot=True, square=True, cbar=False, cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels', size=18)\n",
        "plt.ylabel('True Labels',size=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTTeI9WNT68P",
        "colab": {}
      },
      "source": [
        "#Print metrics\n",
        "print(metrics.classification_report\n",
        "      (y_test,y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "45y5MQyBT68Q"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAEzscyJT68R"
      },
      "source": [
        "Let's recall the definition of each metric shown in `classification_report()`\n",
        "\n",
        "First, for a classification problem we can define <font color='green'>True Positives</font>, <font color='red'>False Positives</font>, <font color='red'>False Negatives</font> and <font color='green'>True Negatives</font>. To understand how we define these values, **let's suppose we are interested in the CLASS 0 objects**. Then:\n",
        "\n",
        "- <font color='green'>True Positives</font>: is the number of CLASS 0 objects that were CORRECTLY classified as CLASS 0 \n",
        "- <font color='red'>False Positives</font>: is the number of CLASS 1 or CLASS 2 objects that were INCORRECTLY classified as CLASS 0 \n",
        "- <font color='red'>False Negatives</font>: is the number of CLASS 0 objects that were INCORRECTLY classified as CLASS 1 or CLASS 2\n",
        "- <font color='green'>True Negatives</font>: is the number of CLASS 1 or CLASS 2 objects that were CORRECTLY or INCORRECTLY classified as CLASS 1 or CLASS2 (for simplicity, we can gather all these cases altogether)\n",
        "\n",
        "\n",
        "We can see below how the confusion matrix looks like:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wg8riQTTT68R"
      },
      "source": [
        "<p align=\"center\"><img src = \"https://raw.githubusercontent.com/marixko/classification_regression/master/confusion_matrix.png\" width=500/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BlWcjxjgT68V"
      },
      "source": [
        "### Precision/Purity\n",
        "\n",
        "\n",
        "> Precision $\n",
        "\\equiv \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$ is the fraction of correct classifications among objects classified as CLASS 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCUxWavVNAVp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Recall/Completeness\n",
        "\n",
        "> Recall $\n",
        "\\equiv \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$ is the fraction of CLASS 0 objects that we are classifying correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qacHsTjcNCzU",
        "colab_type": "text"
      },
      "source": [
        "### F-score \n",
        "\n",
        "> $F = 2\\Big(P_i^{-1}+R_i^{-1}\\Big)^{-1}  = 2 \\times \\frac{P_iR_i}{P_i+R_i}$ is the harmonic mean of Precision and REcall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ1G_JHeNEjp",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "> Accuracy $\\equiv \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total}}$ is the fraction of correct classifications, in overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZkC7pC1XMHn",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**>> TO THINK ABOUT:**</font>\n",
        "\n",
        "Generalize the formulas above for any given class. (HINT: you can define the confusion matrix using double subindex, for instance FN$_{i,i}$ for false negatives and FP$_{i,j}$ for false positives, where $i,j = \\{0,1,2\\}$ ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-zUvQmgT68a"
      },
      "source": [
        "#Regression with Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3WzpBcYOT68a",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86dQ7GWGT68d",
        "colab": {}
      },
      "source": [
        "# Let's read the dataset. We will use this dataset for training and validating models\n",
        "california_housing = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "\n",
        "# This dataset will be used for testing our model\n",
        "california_housing_test = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\", sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mFSr2PKST68f"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RI7Ed9nbT68f"
      },
      "source": [
        "This is the most important part when working with Machine Learning. It is not as simple as a cake recipe, but these few steps will already give you a better understanding of your data. Decisions must be taken from there, case by case. \n",
        "\n",
        "\n",
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "---\n",
        "\n",
        "STEP 1: Check if your dataset is ok \n",
        "- [x] Check names of the columns -> `list(df)` or `print(df.columns)`\n",
        "- [x] Check type of each column  -> `df.dtypes`\n",
        " ...or simply do `df.info()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "STEP 2: Check statistics\n",
        "- [x] Statistical summary -> `df.describe().T` \n",
        "- [x] Check outliers -> `df.column.boxplot()` \n",
        "\n",
        "(+ other analyses based on data visualization )\n",
        "\n",
        "---\n",
        "\n",
        "STEP 3: Check missing values\n",
        "- [ ] Check presence of NaNs -> `df.isna().sum()`\n",
        "- [ ] Drop NaNs, if necessary (not always the best way to deal with it!) -> `df = df.dropna()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "STEP 4: Check correlations\n",
        "- [x] Correlation matrix -> `df.corr()`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UwIL7MeMT68g"
      },
      "source": [
        "<font color='red'>**>> TO THINK ABOUT:**</font>\n",
        "\n",
        "- What response variable (also called as target or dependent variable) do we want to predict? \n",
        "\n",
        "- In your judgement -- based on the plots -- which features (also called as independent variables) would you include to predict the value you are interested in?\n",
        "\n",
        "- What might be the most important independent variable(s) that explain your target? Why?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YeqOEKe3T68g"
      },
      "source": [
        "## Training and Validating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xuNF7gd-T68h"
      },
      "source": [
        "First of all, let's save all features we are going to use to train our algorithm in a variable called **data** and the corresponding price values in a variable called **target**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3newo-WoT68i",
        "colab": {}
      },
      "source": [
        "target = california_housing['median_house_value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-bfpcRZT68s",
        "colab": {}
      },
      "source": [
        "data = california_housing.drop('median_house_value', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78cQjpK7T68t"
      },
      "source": [
        "We can again sample our data into training and validation sets using **Holdout method**. Note that this time we have sufficient data to separate into training, validation and testing samples.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/holdout_2.png\" width=500/></p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVm9rGpzT68t",
        "colab": {}
      },
      "source": [
        "X_train,X_vali,y_train,y_vali=train_test_split(data,target,test_size=0.3,random_state=42)\n",
        "print(len(X_vali), len(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etU7AOEMT68v"
      },
      "source": [
        "Here we also show how we would sample using K-Fold method with (n = 4) folds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jUyHvciT68v",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# kf = KFold(n_splits=4)\n",
        "# for train_index, test_index in kf.split(data):\n",
        "#   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "#   X_train, X_test = data.loc[train_index], data.loc[test_index]\n",
        "#   y_train, y_test = target.loc[train_index], target.loc[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IrcLvKs4T68x"
      },
      "source": [
        "## Training Models and Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JlAJQHZrT68x"
      },
      "source": [
        "### Multiple Linear Regression with Ordinary Least Squares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7rNQ4eH4T68y"
      },
      "source": [
        "The class `LinearRegression()` from sklearn implements the Ordinary Least Squares to estimate the parameters of a linear regression. This method minimizes the residual sum of squares:\n",
        "\n",
        "$RSS(\\beta) = \\sum_{i=1}^{N}(y_i-\\hat{y}_i)$, \n",
        "\n",
        "where y are the observables of the dependent variable (or \"true values\") and $\\hat{y}_i = {\\beta_0+\\beta_1x_1+\\beta_2x_2+...}$ refers to our linear model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9WrCEtHT68y",
        "colab": {}
      },
      "source": [
        "model_LR = LinearRegression().fit(X_train, y_train) #fitting model\n",
        "y_LR = model_LR.predict(X_vali) #predicting\n",
        "\n",
        "# Plotting results\n",
        "plt.close()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_vali, y_LR, alpha=0.5)\n",
        "plt.plot([0,700000], [0,700000], color=\"red\")\n",
        "plt.ylabel('Prediction')\n",
        "plt.xlabel('True')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HaA9QTrGT685",
        "colab": {}
      },
      "source": [
        "# Printing metric\n",
        "print('Mean Squared Error:', mean_squared_error(y_vali,y_LR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yfzwmy0oT688"
      },
      "source": [
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "  \n",
        "- [ ] Use the log (`np.log()`) of the house prices. How does it change your results?\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-dlr6-0T689"
      },
      "source": [
        "### Multiple Linear Regression with Stochastic Gradient Descent \n",
        "\n",
        "Instead of minimizing the residual sum of squares, Stochastic Gradient Descent will minimize the regularized training error given by:\n",
        "\n",
        "$ E(\\beta,b) = \\frac{1}{n} \\sum_{i=1}^{N} L(y_i, \\hat{y_i})+ \\alpha R(\\beta) $,\n",
        "\n",
        "where L is a loss function, R is a regularization term that penalizes model complexity as a function of model parameters w, $\\alpha$ is a non-negative hyperparameter and $\\hat{y}_i = {\\beta_0+\\beta_1x_1+\\beta_2x_2+...}$ refers to our linear model.\n",
        "\n",
        "Popular choices for R:\n",
        "\n",
        "    * L2 norm: \n",
        "$ R(w) = \\frac{1}{2} \\sum_{i=1}^n \\beta_i^2 $ \n",
        "\n",
        "    * L1 norm:\n",
        "$ R(w) = \\sum_{i=1}^n |\\beta_i| $     \n",
        "\n",
        "    * Elastic Net:\n",
        "$ R(w) = \\frac{\\rho}{2} \\sum_{i=1}^n \\beta_i^2 + (1-\\rho) \\sum_{i=1}^n |\\beta_i|  $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBmU4HSRT68-"
      },
      "source": [
        "**IMPORTANT:** SGD is highly sensitive to feature scaling. It is recommended that you scale your data first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APN-zyoPT68-",
        "colab": {}
      },
      "source": [
        "# Scaling your data with standardized normalization (mean 0 and variance 1)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  \n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_vali_scaled = scaler.transform(X_vali)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z1GkTOpFT68_",
        "colab": {}
      },
      "source": [
        "# Fitting model\n",
        "model_SGD = SGDRegressor(loss=\"squared_loss\", penalty=\"l2\", max_iter=1000, random_state=42, eta0=0.01, verbose=1)\n",
        "model_SGD.fit(X_train_scaled, np.log(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzrIjVW_T69B",
        "colab": {}
      },
      "source": [
        "# Predicting\n",
        "y_SGD = model_SGD.predict(X_vali_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G6OdhBauT69C",
        "colab": {}
      },
      "source": [
        "# Plotting results\n",
        "plt.close()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(np.log(y_vali), y_SGD, alpha=0.5)\n",
        "plt.plot([10,15], [10,15], color=\"red\")\n",
        "plt.ylabel('Prediction')\n",
        "plt.xlabel('True')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZnhEoUEsT69J",
        "colab": {}
      },
      "source": [
        "# Printing metric\n",
        "print('Mean Squared Error:', mean_squared_error(np.log(y_vali),y_SGD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6E44-daks6_",
        "colab_type": "text"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_gDejJooOzf",
        "colab_type": "text"
      },
      "source": [
        "- Mean Squared Error: \n",
        "\n",
        "$$ MSE = \\frac{1}{n} \\sum_i (y_i - \\bar{y_i})^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TA0EYkxvT684"
      },
      "source": [
        "If we are interested in acquiring the best model that fits your data, the best way to do model validation is NOT  through cross-validation, but through $R^2$ and/or residual analysis! When using cross-validation, we are focused in getting the best prediction accuracy. Validating in different ways, may lead to different models (i.e., parameters may not be the same!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueg5ihPlkqTB",
        "colab_type": "text"
      },
      "source": [
        "- R$^2$ (coefficient of determination) measures how well the model fits the data. Considering $\\bar{y}$ as being the mean value of your response variable:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr9J8KDEjVBU",
        "colab_type": "text"
      },
      "source": [
        "$$ R^2 = 1 - \\frac{\\sum_n (y_i - \\hat{y}_i)^2}{\\sum_n (y_i - \\bar{y}_i)^2} $$ \n",
        "\n",
        " - if you have multiple features, you must use the adjusted R$^2$: \n",
        "\n",
        " $$ R^2 (adjusted) = 1 - \\left[\\frac{(1-R^2)(n-1)}{n-k-1}\\right],$$ where k is the number of features and n is the sample size.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-aIFO77MiFT",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "1) Based on the results, what model do you choose? Justify\n",
        "\n",
        "2) Train again your chosen model with the entire california_housing dataset (i.e. training + validation sets)\n",
        "\n",
        "3) Make predictions for california_housing_test and print Mean Squared Error. These values estimate the performance of your final model.\n",
        "\n",
        "4) Get your final model by training the algorithm with all your data (i.e. training + validation + test sets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hZ1a2L3bA11",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'> **Exercises with S-PLUS data** </font>\n",
        "\n",
        " Read [splus_laplata.txt](https://github.com/marixko/machine_learning_IXLAPIS/raw/master/splus_laplata.txt) and use `.info()` to check your data\n",
        "\n",
        "**CLASSIFICATION: STAR/GALAXY separation**\n",
        "\n",
        "1) Select all stars (class == STAR) and galaxies (class == GALAXY) and save them in a dataframe named `stargal`\n",
        "\n",
        "2) Use `stargal.class.value_counts()` to check how many stars and galaxies you have in your data. Is it balanced or imbalanced? \n",
        "\n",
        "3) Use `.hist()` and `.pairplot()` (or `.corr()`) to check distributions and correlations. You must plot stars and galaxies in different colors. (HINT: use parameter `c = stargal['class']`). For which features do stars and galaxies behave differently? \n",
        "\n",
        "4) Sample 75% of your dataset for your training set using `train_test_split()`. Save the remaining 25% to the final test. \n",
        "\n",
        "5) Choose at least one algorithm between:\n",
        " \n",
        "  - SVM\n",
        "\n",
        "  - Logistic Regression\n",
        "\n",
        "  - k-Nearest Neighbors\n",
        "\n",
        "6) Choose which features you want to include in the model. (Important: Don't include the errors in the measurements in your model! Of course, don't include the class variable either. )\n",
        "\n",
        "7) Train your model and validate with `classification_report()`. \n",
        "\n",
        "8) Repeat Item 9 for different models. You can change the hyperparameters values (read the documentation to understand them!), the features you are including or even the algorithm. \n",
        "\n",
        "9) Suppose that you want the purest sample of galaxies in order to make statistical inferences about this class. Which model would you select? \n",
        "\n",
        "CHALLENGE: Use K-Fold instead of Holdout for cross-validation. Perform a star/quasar/galaxy separation (the truth table is imbalanced, be careful!).\n",
        "\n",
        "\n",
        "**REGRESSION: Photo-z estimation**\n",
        "\n",
        "1) Select all galaxies (class == GALAXY) and save it in a dataframe named `stargal`\n",
        "\n",
        "2) Use `.hist()` and `.pairplot()` (or `.corr()`) to check distributions and correlations. \n",
        "\n",
        "3) Sample 75% of your dataset for your training set using `train_test_split()`. Save the remaining 25% to the final test. Sample 20% of your training set to be your validation set.\n",
        "\n",
        "4) Train a Linear Regression with Ordinary Least Squares and another with Stochastic Gradient Descent with all S-PLUS magnitudes as input features. Your response variable is the spectroscopic redshift (z). \n",
        "\n",
        "5) Validate your models with the metric(s) of your choice (https://scikit-learn.org/stable/modules/model_evaluation.html - try to understand the metric before using it!). Choose your best model. \n",
        "\n",
        "6) Based on the results of Item 5, could you still improve your model? If yes, try other parametrizations. \n",
        "\n",
        "7) Train again your model with training+validation sets and estimate the performance by calculating the same metric(s) from Item 5 on your testing set. \n",
        "\n",
        "CHALLENGE: Try other algorithms, like LASSO or Ridge regression. Try finding the best learning rate using `GridSearchCV`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNcCsXmEy7m",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h0dXgW0Jp6g",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC12GQrOJua6",
        "colab_type": "text"
      },
      "source": [
        "Keras (https://keras.io/) is a Application Programming Interface (API) which aims to facilitate the interaction between the user and a low-level machine learning language, like TensorFlow (https://www.tensorflow.org/).\n",
        "\n",
        "It was created and is maintained by Francois Chollet who is also the author of the excellent book \"Deep Learning with Python\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7u6VTzcJxQT",
        "colab_type": "text"
      },
      "source": [
        "### Quick recap - how does deep-learning work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4g1qrZuJ1Gf",
        "colab_type": "text"
      },
      "source": [
        "Deep-learning (and machine learning in general) is based on the assumption that there is a relation between the different variables of the problem you are trying to solve.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/Images/TrainLoop.png\" width=500/></p>\n",
        "\n",
        "***\n",
        "To train a model, we need three datasets:\n",
        "* A training dataset: So the model can learn the relation between the input and output;\n",
        "* A validation dataset: So we can have an independent estimate of the performance of the model during the training;\n",
        "* A testing dataset: To make a final evaluation of the performance of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoFRMvS1KDrU",
        "colab_type": "text"
      },
      "source": [
        "### Regression with Keras (Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_HhvrxYHzEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing everything we will need in this part of the hands-on\n",
        "\n",
        "# To make plots\n",
        "import matplotlib.pyplot as plt\n",
        "# To reshape some arrays\n",
        "import numpy as np\n",
        "# To do deep-learning\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "# To import data\n",
        "import pandas as pd\n",
        "\n",
        "# To split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# To normalize the data\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD5hIoaDKIhj",
        "colab_type": "text"
      },
      "source": [
        "#### Loading and defining the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ZghzkGKM9a",
        "colab_type": "text"
      },
      "source": [
        "For the regression part of the hands-on, we will use the (already seen) California housing dataset.\n",
        "\n",
        "This dataset contains 20000 rows in total, with 9 columns each. The features are:\n",
        "* Latitude;\n",
        "* Longitude;\n",
        "* Housing median age;\n",
        "* Total rooms;\n",
        "* Total bedrooms;\n",
        "* Population;\n",
        "* Households;\n",
        "* Median income;\n",
        "* Median house value.\n",
        "\n",
        "For this exercise, we will use the first 8 features to predict the __median house value__ on the testing dataset.\n",
        "\n",
        "Lets start by loading the data with the Pandas package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7QjPbIEJo-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the training dataset with Pandas:\n",
        "Train_Data = pd.read_csv('https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv')\n",
        "\n",
        "# And the testing dataset:\n",
        "Test_Sample = pd.read_csv('https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egStEWbdKbXu",
        "colab_type": "text"
      },
      "source": [
        "As we just saw, we need to define a __validation dataset__ before training our model. This can be easily done with sklearn's 'train_test_split' function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1_egC2-KgRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use this function like this:\n",
        "Train_Sample, Validation_Sample = train_test_split(Train_Data, train_size=0.8, random_state=1882)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYHguTJnKh3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets check the size of each sample:\n",
        "print('Training sample: %s rows'   %(len(Train_Sample)))\n",
        "print('Validation sample: %s rows' %(len(Validation_Sample)))\n",
        "print('Test sample: %s rows'       %(len(Test_Sample)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-eQJb2QKkNU",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<font size=\"3\">\n",
        "When working with a ML problem, we should always check the distribution of the input variables. In this example we will check how the 8 features are distributed in the training, validation, and testing samples.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xwWaHunKmG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The features of our data are:\n",
        "Features = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']\n",
        "\n",
        "# The target of our regression is:\n",
        "Target   = ['median_house_value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuNA83yRKnqU",
        "colab_type": "text"
      },
      "source": [
        "We can check the distribution of features of each dataset using histograms made with Matplotlib. Just run the cell below (you don't need to see the code):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnrl4V1vKqAy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "def sample_histograms(Train, Validation, Test, Features, Target):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "  plt.style.use('default')\n",
        "  plt_idx = 1\n",
        "  for feature in (Features + Target):\n",
        "      plt.subplot(3, 3, plt_idx)\n",
        "\n",
        "      Feature_min = min(np.min(Train[feature]), np.min(Validation[feature]), np.min(Test[feature]))\n",
        "      Feature_max = max(np.max(Train[feature]), np.max(Validation[feature]), np.min(Test[feature]))\n",
        "\n",
        "      plt.hist(Train[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Training sample')\n",
        "      plt.hist(Validation[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Validation sample')\n",
        "      plt.hist(Test[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Testing sample')\n",
        "\n",
        "      plt.yscale('log')\n",
        "\n",
        "      plt.xlabel(feature, size=12)\n",
        "\n",
        "      plt.grid(lw=.5)\n",
        "      plt_idx = plt_idx+1\n",
        "\n",
        "      if plt_idx == 4:\n",
        "        plt.legend(loc='center', bbox_to_anchor=(1.55, 0.845), fontsize=12)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy_3fSEuKsBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_histograms(Train_Sample, Validation_Sample, Test_Sample, Features, Target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpIxJmsLKubl",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioi3EtQCKyWO",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the histograms, each feature spans a different range of values:\n",
        "\n",
        "*   Longitude has negative values;\n",
        "*   Population is in the order of tens of thousands\n",
        "*   Median house value can reach hundreds of thousands\n",
        "\n",
        "We need to standardize our dataset before presenting it to the model in order to facilitate the training (and convergence).\n",
        "\n",
        "To do this, we use sklearn's StandardScaler function.\n",
        "***\n",
        "The StandardScaler function subtracts the mean $\\mu$ of each column and divides by its stardard deviation $\\sigma$, so that all columns have a mean of zero and stardard deviation of unity. Mathematically, it is described as:\n",
        "\\begin{equation}\n",
        "X' = \\frac{(X - \\mu)}{\\sigma},\n",
        "\\end{equation}\n",
        "where $X'$ is the new column and $X$ is the old column. This process occurs inplace, so the new column will replace the old one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8b70bO7K0mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explain why we dont use fit transform in validation and testing\n",
        "\n",
        "# Create our scaling function with default values:\n",
        "Scaler_X = StandardScaler()\n",
        "Scaler_Y = StandardScaler()\n",
        "\n",
        "# Scaling our training data:\n",
        "Scaled_Train_X = Scaler_X.fit_transform(Train_Sample[Features])\n",
        "Scaled_Train_Y = Scaler_Y.fit_transform(Train_Sample[Target])\n",
        "\n",
        "# Now we use the values obtained by the fit on the training data to scale our validation data:\n",
        "Scaled_Validation_X = Scaler_X.transform(Validation_Sample[Features])\n",
        "Scaled_Validation_Y = Scaler_Y.transform(Validation_Sample[Target])\n",
        "\n",
        "# And the testing data\n",
        "Scaled_Test_X = Scaler_X.transform(Test_Sample[Features])\n",
        "Scaled_Test_Y = Scaler_Y.transform(Test_Sample[Target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfBTsxbjK2Z0",
        "colab_type": "text"
      },
      "source": [
        "Now we can finally start creating our model!\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp6Q7NUSGatO",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a deep-learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YrUWXG2K_CT",
        "colab_type": "text"
      },
      "source": [
        "The process of creating a model with Keras is really simple, it is almost as if we were playing with Legos.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/Images/KerasLego.png\" width=500/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkWnTD6TLFgt",
        "colab_type": "text"
      },
      "source": [
        "We start by defining the method we want to use to create our network. The options are:\n",
        "1. Sequential: Allows to create the network layer-by-layer. It's the easiest way to create a neural network, but does not allow the creation of shared layers or networks with multiple inputs and/or outputs.\n",
        "2. Functional: A little more complex than the Sequential API, but still easy to use. Offers the capability of creating networks with branches and multiple inputs and/or outputs.\n",
        "3. Model Subclassing: Way more dificult to use, but allows the creation of fully customizable and complex architectures, including custom layers and different ways to treat the data.\n",
        "***\n",
        "For this hands-on, we will use the Sequential method.\n",
        "\n",
        "We start defining our network by selecting the method we will use, using ```model = Sequential()``` in our case. Then we add layers the way we want using the ```model.add(...)``` function and the final step is to compile the model using ```model.compile(...)```.\n",
        "\n",
        "***\n",
        "***Hands-on***: Complete the model below the way you want. \n",
        "1. Add two or three Dense layers with any number of neurons between 16 and 128 with a 'relu' activation.\n",
        "    * Try to use multiples of 2 neurons (16, 32, 64, 128, ...)\n",
        "2. Add a Dropout layer between the Dense layers. Try a dropout rate between 0.1 and 0.3.\n",
        "3. Choose the parameters to compile the model.\n",
        "    * Loss: 'mean_squared_error' or 'mean_absolute_deviation' will work nicely.\n",
        "    * Metrics: In case you want to monitor other information, you can define it here as a list (metrics=['mean_absolute_deviation'], for example).\n",
        "    * Optimizer: How the network will optimize the weights. You can choose from 'adam', 'nadam', 'rmsprop', or 'sgd', among others.\n",
        "***\n",
        "***Tips***\n",
        "* A Dense layer is written as ```Dense(units=n, activation='name')```.\n",
        "* A Dropout layer is written as ```Dropout(rate=n)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjDGDoxtLJJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some names to make the notation friendlier\n",
        "\n",
        "Sequential = tf.keras.Sequential\n",
        "Dense      = tf.keras.layers.Dense\n",
        "Dropout    = tf.keras.layers.Dropout\n",
        "\n",
        "def My_Model():\n",
        "    # Define a Sequential model:\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input Layer\n",
        "    model.add(Dense(units=8, input_dim=8, activation='linear'))\n",
        "\n",
        "    # Hidden Layers\n",
        "    model.add(...)\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=..., \n",
        "                  optimizer=...)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Lets see a summary of our model:\n",
        "My_Model().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "676vDTsVG75c",
        "colab_type": "text"
      },
      "source": [
        "##### Creating a model ('easy' mode) using forms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw27muG7HA6Y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Defining a model\n",
        "hidden_layers = 3 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "hidden_neurons = 64 #@param {type:\"slider\", min:16, max:256, step:16}\n",
        "use_dropout = True #@param {type:\"boolean\"}\n",
        "dropout_rate = 0.2 #@param {type:\"slider\", min:0.1, max:0.9, step:0.1}\n",
        "loss_function = \"mean_squared_error\" #@param [\"mean_squared_error\", \"mean_absolute_error\"]\n",
        "optimizer_function = \"rmsprop\" #@param [\"adam\", \"nadam\", \"rmsprop\", \"sgd\"]\n",
        "\n",
        "# Defining some names to make the notation friendlier\n",
        "\n",
        "Sequential = tf.keras.Sequential\n",
        "Dense      = tf.keras.layers.Dense\n",
        "Dropout    = tf.keras.layers.Dropout\n",
        "\n",
        "def My_Model():\n",
        "    # Define a Sequential model:\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input Layer\n",
        "    model.add(Dense(units=8, input_dim=8, activation='linear'))\n",
        "\n",
        "    # Hidden Layers\n",
        "    for layer in range(hidden_layers):\n",
        "      model.add(Dense(units=hidden_neurons, activation='relu'))\n",
        "      if use_dropout == True:\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=loss_function, \n",
        "                  optimizer=optimizer_function)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Lets see a summary of our model:\n",
        "My_Model().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu8aGNZrLSyW",
        "colab_type": "text"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbCaooeyLU4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model we just defined:\n",
        "MLP_Model = My_Model()\n",
        "\n",
        "# Fitting the model\n",
        "MLP_Model_Fit = MLP_Model.fit(Scaled_Train_X, Scaled_Train_Y, \n",
        "                              validation_data=[Scaled_Validation_X, Scaled_Validation_Y],\n",
        "                              epochs=100, batch_size=256, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY3DpHRhLYH4",
        "colab_type": "text"
      },
      "source": [
        "When the model is trained, we can easily save it to use later with\n",
        "```\n",
        "MLP_Model.save('Saved_Model.h5')\n",
        "```\n",
        "\n",
        "To load it later and make prediction normally, we use the 'load_model' function:\n",
        "```\n",
        "Loaded_Model = tf.keras.models.load_model('Saved_Model.h5')\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "Now lets verify if the training process went well by analysing the loss plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_WTjYXeLa6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(7,5))\n",
        "\n",
        "X = MLP_Model_Fit.epoch\n",
        "\n",
        "plt.plot(X, MLP_Model_Fit.history['loss'], lw=3, label='Training')\n",
        "plt.plot(X, MLP_Model_Fit.history['val_loss'], lw=3, label='Validation')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Rweo2_LirI",
        "colab_type": "text"
      },
      "source": [
        "#### Making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5phksHfLl-d",
        "colab_type": "text"
      },
      "source": [
        "Making predictions are as simple as calling a function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmnbNe4YLncw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions for the test set:\n",
        "Predictions = MLP_Model.predict(Scaled_Test_X)\n",
        "\n",
        "# Since we used the StandardScaler function to normalize all data, we need to do the inverse transform to get the actual house values:\n",
        "Predictions = Scaler_Y.inverse_transform(Predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wBLsfb9LqY6",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can compare our predictions against the real house values on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp3o0rXCJsJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Delta = (Predictions - Test_Sample[Target]).values\n",
        "Sigma = np.std(Delta)\n",
        "Bias  = np.median(Delta)\n",
        "Outlier_Fraction = 100*(np.count_nonzero(np.abs(Delta) >= 3*Sigma)/len(Test_Sample))\n",
        "\n",
        "print('Standard deviation: %s \\nBias: %s \\nOutlier_Fraction: %.2f' %(Sigma, Bias, Outlier_Fraction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX1P-Z7fLtqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(7,7))\n",
        "plt.hexbin(Test_Sample[Target], Predictions, bins='log', cmap='Greys', gridsize=50)\n",
        "\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot([0,500000], [0,500000], '--', color='red', label='Perfect predition')\n",
        "plt.axis([0,500000, 0, 500000])\n",
        "\n",
        "x = np.arange(1, 510000, 10000)\n",
        "plt.fill_between(x=x, y1=x+3*Sigma, y2=500000, color='red', alpha=0.2, label='Outlier region')\n",
        "plt.fill_between(x=x, y1=x-3*Sigma, y2=0,      color='red', alpha=0.2)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRL_7q8aL1sl",
        "colab_type": "text"
      },
      "source": [
        "### Classification with Keras (Convolutional Neural Networks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg0VD7A-8bxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing everything we will need in this part of the hands-on\n",
        "\n",
        "# To make plots\n",
        "import matplotlib.pyplot as plt\n",
        "# To reshape some arrays\n",
        "import numpy as np\n",
        "# To do deep-learning\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# To plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# To split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# To normalize the data\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUagJbKEL7D6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ur-q2oTL91r",
        "colab_type": "text"
      },
      "source": [
        "For this part of the lesson, we will use the MNIST dataset. This dataset contains 70000 rows, each one representing the image of a handwritten number.\n",
        "\n",
        "The dataset contains:\n",
        "* One label column, with the number represented in the image;\n",
        "* 784 (28x28) columns with the information of the pixels in the image.\n",
        "\n",
        "Each image has 28x28 pixels, and the value of each pixel ranges from 0 to 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcxyPA3RL_y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the training dataset and testing datasets directly from TensorFlow (the are built-in):\n",
        "(Train_Data, Train_Labels), (Test_Sample, Test_Labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_agLi1BMD8D",
        "colab_type": "text"
      },
      "source": [
        "##### Seeing the image (example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otixsLVWMGqh",
        "colab_type": "text"
      },
      "source": [
        "To see the images, we need to do some extra steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7Zl52DsMH8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each row of these datasets represents an image. We need to convert the numbers to an image to visualize it\n",
        "# First, we select the first row of our dataset and all elements after the very first one (which is the label of the image), then we reshape it to a 28x28 format:\n",
        "idx   = np.random.randint(0, 60000) # To print a different image each time you run this cell\n",
        "Image = Train_Data[idx].reshape(28,28)\n",
        "\n",
        "# Now, we just need to use Matplotlib's 'imshow' function\n",
        "plt.imshow(Image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noxaXAXQMQND",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csBnrg1NMTN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just like we did in the regression part, we should now define a new validation dataset using sklearn's 'train_test_split' function:\n",
        "# We use this function like this:\n",
        "Train_Sample, Validation_Sample, Train_Labels, Validation_Labels = train_test_split(Train_Data, Train_Labels, train_size=0.8, random_state=1882)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u7CDn2GMUpt",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qwdSrSiMazI",
        "colab_type": "text"
      },
      "source": [
        "Instead of using the StandardScaler function, we will use the MinMaxScaler function.\n",
        "The MinMaxScaler is a little different. This function brings the values of a column to the range $[0, 1]$ by doing two steps:\n",
        "- 1st step: Calculate the scale: $\\text{Scale} = (\\text{max} - \\text{min}) / (X_\\text{max} - X_\\text{min})$\n",
        "- 2nd step: Apply the transformation: $X' = \\text{Scale} \\times X + \\text{min} - X_\\text{min} \\times \\text{Scale}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWBE6ysVMeFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create our scaling function with default values:\n",
        "Scaler = MinMaxScaler()\n",
        "\n",
        "# Scaling our training data:\n",
        "Scaled_Train_X      = Scaler.fit_transform(Train_Sample.reshape(len(Train_Sample), 28*28))\n",
        "\n",
        "# Scaling our validation data:\n",
        "Scaled_Validation_X = Scaler.transform(Validation_Sample.reshape(len(Validation_Sample), 28*28))\n",
        "\n",
        "# Now we use the values obtained by the fit on the training data to scale our testing data:\n",
        "Scaled_Test_X       = Scaler.transform(Test_Sample.reshape(len(Test_Sample), 28*28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6rsBnIwMgg7",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "A Convolutional Neural Network works best with images with three dimensions (height, width and depth), but our data is composed of 1D vectors. We need to reshape the data before handling it to our model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BHmBjstMibF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the training sample, we first create and empty list:\n",
        "Training_Images = []\n",
        "\n",
        "# Now we will loop over all rows in our dataset\n",
        "for i in range(len(Scaled_Train_X)):\n",
        "  # The row 'i' will be reshaped to form a 28x28 array:\n",
        "  Reshaped_Data = Scaled_Train_X[i].reshape(28,28)\n",
        "  # And we append the reshaped row to the list we just created:\n",
        "  Training_Images.append(Reshaped_Data)\n",
        "\n",
        "# We reshaped our 1D data into a 2D image, but CNNs use 3D images as input (height, width and depth), so we use the 'expand_dims' to turn our images into 3D vectors.\n",
        "Training_Images = np.expand_dims(Training_Images, axis=3)\n",
        "  \n",
        "# Repeat for the validation sample:\n",
        "Validation_Images = []\n",
        "\n",
        "for i in range(len(Scaled_Validation_X)):\n",
        "  Reshaped_Data = Scaled_Validation_X[i].reshape(28,28)\n",
        "  Validation_Images.append(Reshaped_Data)\n",
        "  \n",
        "Validation_Images = np.expand_dims(Validation_Images, axis=3)\n",
        "  \n",
        "# And for the testing sample:\n",
        "Testing_Images = []\n",
        "\n",
        "for i in range(len(Scaled_Test_X)):\n",
        "  Reshaped_Data = Scaled_Test_X[i].reshape(28,28)\n",
        "  Testing_Images.append(Reshaped_Data)\n",
        "  \n",
        "Testing_Images = np.expand_dims(Testing_Images, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xOl6ERjMlQ8",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "The task we have at hand is considered a 'multi-class' classification because we want to classify images in numbers between 0 and 9. For this reason we need to 'one hot encode' our label vectors.\n",
        "\n",
        "Why we need to do this 'one hot encoding'? Simple, when the neural networks sees categorical labels (from 0 to 9) it 'thinks' that 9 is the most important class, followed by 8 and so on. Due to this we transform these categorical labels into a one hot vector, like this:\n",
        "\n",
        "***\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "\\textbf{Categorical label}\\\\ \\hline\n",
        "1\\\\ \n",
        "8\\\\ \n",
        "5\\\\ \n",
        "3\\\\ \n",
        "2\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{0} &\\mathbf{1}  &\\mathbf{2}  &\\mathbf{3}  &\\mathbf{4}  &\\mathbf{5}  &\\mathbf{6}  &\\mathbf{7}  &\\mathbf{8}  &\\mathbf{9} \\\\ \\hline\n",
        "0 &1  &0  &0  &0  &0  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &0  &0  &0  &0  &0  &0  &1  &0 \\\\ \n",
        "0 &0  &0  &0  &0  &1  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &0  &1  &0  &0  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &1  &0  &0  &0  &0  &0  &0  &0 \n",
        "\\end{bmatrix}\n",
        "$\n",
        "***\n",
        "\n",
        "Now the network will not give more importance to one label over another!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-sKJ3HhNGsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the training sample\n",
        "Train_Labels = tf.keras.utils.to_categorical(Train_Labels)\n",
        "\n",
        "# For the validation sample\n",
        "Validation_Labels = tf.keras.utils.to_categorical(Validation_Labels)\n",
        "\n",
        "# For the testing sample\n",
        "Test_Labels = tf.keras.utils.to_categorical(Test_Labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7yEzENKNJmT",
        "colab_type": "text"
      },
      "source": [
        "We can finally create our model, just like we did before, with a few changes in the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fa-eAeVNMIS",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2xF6uONOoz",
        "colab_type": "text"
      },
      "source": [
        "The main difference between a multi-layer perceptron and a convolutional neural network is the presence of convolutional layers.\n",
        "\n",
        "The main configuration of Dense layers are the number of neurons and the activation function. For Conv2D layers, we need:\n",
        "* The number of filters;\n",
        "* The kernel size;\n",
        "* The stride;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31yKilbwNRuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some names to make the notation friendlier\n",
        "\n",
        "Sequential   = tf.keras.Sequential\n",
        "Dense        = tf.keras.layers.Dense\n",
        "Dropout      = tf.keras.layers.Dropout\n",
        "Conv2D       = tf.keras.layers.Conv2D\n",
        "Flatten      = tf.keras.layers.Flatten\n",
        "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def My_Model():\n",
        "    # Define a Sequential model:\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input Layer\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "\n",
        "    # Hidden Layers\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    # The final part of a CNN is a group of fully-connected layers (Dense), so the networks can join all the information learned and give an output. \n",
        "    # The 'Flatten' layer is responsible for reshaping the information from the Conv2D layers (which are 3D) into the format that Dense layers use (which is 1D).\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'], \n",
        "                  optimizer='nadam')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Lets see a summary of our model:\n",
        "My_Model().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gul9_qkENUCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model we just defined:\n",
        "CNN_Model = My_Model()\n",
        "\n",
        "# Fitting the model\n",
        "CNN_Model_Fit = CNN_Model.fit(Training_Images, Train_Labels,\n",
        "                              validation_data=[Validation_Images, Validation_Labels],\n",
        "                              epochs=2, batch_size=256, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbVK9FzFNVns",
        "colab_type": "text"
      },
      "source": [
        "#### Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQCpf5h2NXN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions for the test set:\n",
        "Predictions = CNN_Model.predict(Testing_Images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjxmWq-sNZ7s",
        "colab_type": "text"
      },
      "source": [
        "To check if (or how much) our predictions are correct, we can use the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQx1ALnNarZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then we change our prediction vector a bit, so it is in the format that the 'confusion_matrix' function accepts:\n",
        "Test_True_Labels = np.argmax(Test_Labels, axis=1)\n",
        "Test_Pred_Labels = np.argmax(Predictions, axis=1)\n",
        "\n",
        "# We calculate the confusion matrix:\n",
        "Conf_Matrix = confusion_matrix(Test_True_Labels, Test_Pred_Labels)\n",
        "\n",
        "# And we finally plot it using Seaborn:\n",
        "import seaborn as sn\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "sn.set(font_scale=1.4)\n",
        "sn.heatmap(Conf_Matrix, annot=True, fmt='', annot_kws={\"size\": 12}, cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woXx6ril94B5",
        "colab_type": "text"
      },
      "source": [
        "#### Extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVdbH7QB3nqn",
        "colab_type": "text"
      },
      "source": [
        "Run the cell below (you don't need to see the code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw5jUFrfZXLm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<canvas width=%d height=%d></canvas>\n",
        "<button>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def draw(filename='drawing.jpg', w=200, h=200, line_width=15):\n",
        "  display(HTML(canvas_html % (w, h, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return len(binary)\n",
        "\n",
        "def draw_and_predict(filename='drawing.jpg', w=200, h=200, line_width=15):\n",
        "  display(HTML(canvas_html % (w, h, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  \n",
        "  import cv2\n",
        "  img = cv2.imread('drawing.jpg', -1)\n",
        "  res = cv2.resize(img, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  print(\"This is what you've had drawn, but resized to 28x28:\")\n",
        "  cv2_imshow(res)\n",
        "\n",
        "  # make mask of where the transparent bits are\n",
        "  trans_mask = res[:,:,3] == 0\n",
        "  # replace areas of transparency with white and not transparent\n",
        "  res[trans_mask] = [255, 255, 255, 255]\n",
        "  # new image without alpha channel...\n",
        "  new_img = cv2.cvtColor(res, cv2.COLOR_BGRA2BGR)\n",
        "  # Inverting colors to predict\n",
        "  new_img = cv2.bitwise_not(new_img)\n",
        "  print(\"This is what the network will see:\")\n",
        "  cv2_imshow(new_img)\n",
        "\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  test_im = new_img[:,:,0]\n",
        "  test_im = np.expand_dims(test_im, 3)\n",
        "  prediction = CNN_Model.predict(test_im.reshape(1, 28, 28, 1)/255)\n",
        "  prediction_idx = np.argmax(prediction)\n",
        "\n",
        "  print(\"The network is %.2f%% sure that you have drawn a %s!\" %((prediction[0][prediction_idx])*100, prediction_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywQa_OjRBuTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_and_predict(line_width=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcwvrB6rNhUo",
        "colab_type": "text"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCnuJyKIXT5k",
        "colab_type": "text"
      },
      "source": [
        "1. Create and train a multi-layer perceptron model to predict photometric redshifts on S-PLUS data.\n",
        "    * Evaluate the performance of your model by calculating the stardard deviation ($\\sigma$), bias ($\\mu$) and fraction of outliers ($\\eta$) of your results.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\delta = (\\text{True values} - \\text{Predicted values})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\sigma = \\sqrt{\\delta/N}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\mu = \\overline{\\delta}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\eta = \\text{Fraction of objects for which} \\delta \\geq 0.15\n",
        "\\end{equation}\n",
        "\n",
        "2. Create a Convolutional Neural Network to classify the Fashion MNIST dataset.\n",
        "    * The dataset is built-in with [Keras](https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles). You can also easily find it on the internet.\n",
        "    * Make a confusion matrix of your results. Your model is predicting correctly?"
      ]
    }
  ]
}